{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d36e1e93-ae93-4a4e-93c6-68fd868d2882",
   "metadata": {},
   "source": [
    "# Using VeRA for sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc0610-55f6-4343-a950-125ccf0f45ac",
   "metadata": {},
   "source": [
    "In this example, we fine-tune Roberta on a sequence classification task using VeRA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45addd81-d4f3-4dfd-960d-3920d347f0a6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "a9935ae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:47:28.134215Z",
     "start_time": "2025-01-15T13:47:28.131710Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] ='https://hf-mirror.com'\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    VeraConfig,\n",
    "    PeftType,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, set_seed, AutoConfig\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "62c959bf-7cc2-49e0-b97e-4c10ec3b9bf3",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "id": "e3b13308",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:47:29.642798Z",
     "start_time": "2025-01-15T13:47:29.639547Z"
    }
   },
   "source": [
    "batch_size = 128\n",
    "model_name_or_path = \"/home/elvin/NAS-Disk-1/WGA/models/roberta-base\"\n",
    "task = \"mrpc\"\n",
    "peft_type = PeftType.VERA\n",
    "device = \"cuda\"\n",
    "num_epochs = 5  # for best results, increase this number\n",
    "rank = 8        # for best results, increase this number\n",
    "max_length = 128\n",
    "torch.manual_seed(0)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7244dffeb0d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "0526f571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:47:31.163414Z",
     "start_time": "2025-01-15T13:47:31.161282Z"
    }
   },
   "source": [
    "peft_config = VeraConfig(\n",
    "    task_type=\"SEQ_CLS\", \n",
    "    r=rank,\n",
    "    target_modules=[\"query\", \"value\", \"intermediate.dense\"],\n",
    "    save_projection=True,\n",
    ")\n",
    "head_lr = 1e-2\n",
    "vera_lr = 2e-2"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "c075c5d2-a457-4f37-a7f1-94fd0d277972",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "id": "7bb52cb4-d1c3-4b04-8bf0-f39ca88af139",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:47:32.887976Z",
     "start_time": "2025-01-15T13:47:32.814744Z"
    }
   },
   "source": [
    "if any(k in model_name_or_path for k in (\"gpt\", \"opt\", \"bloom\")):\n",
    "    padding_side = \"left\"\n",
    "else:\n",
    "    padding_side = \"right\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=padding_side)\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "e69c5e1f-d27b-4264-a41e-fc9b99d025e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:47:39.859005Z",
     "start_time": "2025-01-15T13:47:34.188769Z"
    }
   },
   "source": [
    "datasets = load_dataset(\"/home/elvin/NAS-Disk-1/WGA/datasets/glue\", task)\n",
    "metric = evaluate.load(\"glue\", task)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/elvin/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Thu Jan  9 15:56:57 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "0209f778-c93b-40eb-a4e0-24c25db03980",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:47:44.955304Z",
     "start_time": "2025-01-15T13:47:44.906303Z"
    }
   },
   "source": [
    "def tokenize_function(examples):\n",
    "    # max_length=None => use the model max length (it's actually the default)\n",
    "    outputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, max_length=max_length)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"idx\", \"sentence1\", \"sentence2\"],\n",
    ")\n",
    "\n",
    "# We also rename the 'label' column to 'labels' which is the expected name for labels by the models of the\n",
    "# transformers library\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "7453954e-982c-46f0-b09c-589776e6d6cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:47:54.998251Z",
     "start_time": "2025-01-15T13:47:54.995703Z"
    }
   },
   "source": [
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Instantiate dataloaders.\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, collate_fn=collate_fn, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], shuffle=False, collate_fn=collate_fn, batch_size=batch_size\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "f3b9b2e8-f415-4d0f-9fb4-436f1a3585ea",
   "metadata": {},
   "source": [
    "## Preparing the VeRA model"
   ]
  },
  {
   "cell_type": "code",
   "id": "2ed5ac74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:47:56.788394Z",
     "start_time": "2025-01-15T13:47:56.713951Z"
    }
   },
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True, max_length=None)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /home/elvin/NAS-Disk-1/WGA/models/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"tuple\") to str",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForSequenceClassification\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name_or_path, return_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m get_peft_model(model, peft_config)\n\u001B[1;32m      3\u001B[0m model\u001B[38;5;241m.\u001B[39mprint_trainable_parameters()\n",
      "File \u001B[0;32m~/NAS-Disk-1/WGA/peft/src/peft/mapping.py:230\u001B[0m, in \u001B[0;36mget_peft_model\u001B[0;34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision, low_cpu_mem_usage)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m peft_config\u001B[38;5;241m.\u001B[39mis_prompt_learning:\n\u001B[1;32m    229\u001B[0m     peft_config \u001B[38;5;241m=\u001B[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001B[0;32m--> 230\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config\u001B[38;5;241m.\u001B[39mtask_type](\n\u001B[1;32m    231\u001B[0m     model,\n\u001B[1;32m    232\u001B[0m     peft_config,\n\u001B[1;32m    233\u001B[0m     adapter_name\u001B[38;5;241m=\u001B[39madapter_name,\n\u001B[1;32m    234\u001B[0m     autocast_adapter_dtype\u001B[38;5;241m=\u001B[39mautocast_adapter_dtype,\n\u001B[1;32m    235\u001B[0m     low_cpu_mem_usage\u001B[38;5;241m=\u001B[39mlow_cpu_mem_usage,\n\u001B[1;32m    236\u001B[0m )\n",
      "File \u001B[0;32m~/NAS-Disk-1/WGA/peft/src/peft/peft_model.py:1466\u001B[0m, in \u001B[0;36mPeftModelForSequenceClassification.__init__\u001B[0;34m(self, model, peft_config, adapter_name, **kwargs)\u001B[0m\n\u001B[1;32m   1463\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m   1464\u001B[0m     \u001B[38;5;28mself\u001B[39m, model: torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule, peft_config: PeftConfig, adapter_name: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m   1465\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1466\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(model, peft_config, adapter_name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1468\u001B[0m     classifier_module_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclassifier\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1469\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodules_to_save \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/NAS-Disk-1/WGA/peft/src/peft/peft_model.py:176\u001B[0m, in \u001B[0;36mPeftModel.__init__\u001B[0;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001B[0m\n\u001B[1;32m    174\u001B[0m     ctx \u001B[38;5;241m=\u001B[39m init_empty_weights \u001B[38;5;28;01mif\u001B[39;00m low_cpu_mem_usage \u001B[38;5;28;01melse\u001B[39;00m nullcontext\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx():\n\u001B[0;32m--> 176\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(model, {adapter_name: peft_config}, adapter_name)\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cast_adapter_dtype\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/NAS-Disk-1/WGA/peft/src/peft/tuners/vera/model.py:105\u001B[0m, in \u001B[0;36mVeraModel.__init__\u001B[0;34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, config, adapter_name, low_cpu_mem_usage: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 105\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(model, config, adapter_name, low_cpu_mem_usage\u001B[38;5;241m=\u001B[39mlow_cpu_mem_usage)\n",
      "File \u001B[0;32m~/NAS-Disk-1/WGA/peft/src/peft/tuners/tuners_utils.py:184\u001B[0m, in \u001B[0;36mBaseTuner.__init__\u001B[0;34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001B[0m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pre_injection_hook(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config[adapter_name], adapter_name)\n\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m peft_config \u001B[38;5;241m!=\u001B[39m PeftType\u001B[38;5;241m.\u001B[39mXLORA \u001B[38;5;129;01mor\u001B[39;00m peft_config[adapter_name] \u001B[38;5;241m!=\u001B[39m PeftType\u001B[38;5;241m.\u001B[39mXLORA:\n\u001B[0;32m--> 184\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minject_adapter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, adapter_name, low_cpu_mem_usage\u001B[38;5;241m=\u001B[39mlow_cpu_mem_usage)\n\u001B[1;32m    186\u001B[0m \u001B[38;5;66;03m# Copy the peft_config in the injected model.\u001B[39;00m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mpeft_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpeft_config\n",
      "File \u001B[0;32m~/NAS-Disk-1/WGA/peft/src/peft/tuners/tuners_utils.py:501\u001B[0m, in \u001B[0;36mBaseTuner.inject_adapter\u001B[0;34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001B[0m\n\u001B[1;32m    499\u001B[0m         ctx \u001B[38;5;241m=\u001B[39m init_empty_weights \u001B[38;5;28;01mif\u001B[39;00m low_cpu_mem_usage \u001B[38;5;28;01melse\u001B[39;00m nullcontext\n\u001B[1;32m    500\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m ctx():\n\u001B[0;32m--> 501\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key\u001B[38;5;241m=\u001B[39mkey)\n\u001B[1;32m    503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtargeted_module_names \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m uses_dummy_target_modules:\n\u001B[1;32m    504\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m excluded_modules \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m unmatched_modules:\n\u001B[1;32m    505\u001B[0m         \u001B[38;5;66;03m# All targeted modules were excluded\u001B[39;00m\n",
      "File \u001B[0;32m~/NAS-Disk-1/WGA/peft/src/peft/tuners/vera/model.py:235\u001B[0m, in \u001B[0;36mVeraModel._create_and_replace\u001B[0;34m(self, vera_config, adapter_name, target, target_name, parent, current_key, **optional_kwargs)\u001B[0m\n\u001B[1;32m    226\u001B[0m     target\u001B[38;5;241m.\u001B[39mupdate_layer(\n\u001B[1;32m    227\u001B[0m         adapter_name,\n\u001B[1;32m    228\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvera_A,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    232\u001B[0m         vera_config\u001B[38;5;241m.\u001B[39minit_weights,\n\u001B[1;32m    233\u001B[0m     )\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 235\u001B[0m     new_module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_new_module(vera_config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvera_A, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvera_B, adapter_name, target, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m adapter_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_adapter:\n\u001B[1;32m    237\u001B[0m         \u001B[38;5;66;03m# adding an additional adapter: it is not automatically trainable\u001B[39;00m\n\u001B[1;32m    238\u001B[0m         new_module\u001B[38;5;241m.\u001B[39mrequires_grad_(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/NAS-Disk-1/WGA/peft/src/peft/tuners/vera/model.py:351\u001B[0m, in \u001B[0;36mVeraModel._create_new_module\u001B[0;34m(vera_config, vera_A, vera_B, adapter_name, target, **kwargs)\u001B[0m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    347\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    348\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTarget module \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not supported. Currently, only the following modules are supported: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    349\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`torch.nn.Linear`, `transformers.pytorch_utils.Conv1D`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    350\u001B[0m     )\n\u001B[0;32m--> 351\u001B[0m new_module \u001B[38;5;241m=\u001B[39m Linear(\n\u001B[1;32m    352\u001B[0m     target,\n\u001B[1;32m    353\u001B[0m     vera_A,\n\u001B[1;32m    354\u001B[0m     vera_B,\n\u001B[1;32m    355\u001B[0m     adapter_name,\n\u001B[1;32m    356\u001B[0m     bias\u001B[38;5;241m=\u001B[39mbias,\n\u001B[1;32m    357\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    358\u001B[0m )\n\u001B[1;32m    360\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m new_module\n",
      "File \u001B[0;32m~/NAS-Disk-1/WGA/peft/src/peft/tuners/vera/layer.py:158\u001B[0m, in \u001B[0;36mLinear.__init__\u001B[0;34m(self, base_layer, vera_A, vera_B, adapter_name, r, vera_dropout, fan_in_fan_out, is_target_conv_1d_layer, init_weights, **kwargs)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfan_in_fan_out \u001B[38;5;241m=\u001B[39m fan_in_fan_out\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_active_adapter \u001B[38;5;241m=\u001B[39m adapter_name\n\u001B[0;32m--> 158\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_layer(adapter_name, vera_A, vera_B, r, vera_dropout, init_weights)\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_target_conv_1d_layer \u001B[38;5;241m=\u001B[39m is_target_conv_1d_layer\n",
      "File \u001B[0;32m~/NAS-Disk-1/WGA/peft/src/peft/tuners/vera/layer.py:128\u001B[0m, in \u001B[0;36mVeraLayer.update_layer\u001B[0;34m(self, adapter_name, vera_A, vera_B, r, vera_dropout, init_weights)\u001B[0m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m init_weights:\n\u001B[1;32m    126\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset_vera_parameters(adapter_name)\n\u001B[0;32m--> 128\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_move_adapter_to_device_of_base_layer(adapter_name)\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_adapter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_adapters)\n",
      "File \u001B[0;32m~/NAS-Disk-1/WGA/peft/src/peft/tuners/tuners_utils.py:847\u001B[0m, in \u001B[0;36mBaseTunerLayer._move_adapter_to_device_of_base_layer\u001B[0;34m(self, adapter_name, device)\u001B[0m\n\u001B[1;32m    842\u001B[0m meta \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    844\u001B[0m \u001B[38;5;66;03m# loop through all potential adapter layers and move them to the device of the base layer; be careful to only\u001B[39;00m\n\u001B[1;32m    845\u001B[0m \u001B[38;5;66;03m# move this specific adapter to the device, as the other adapters could be on different devices\u001B[39;00m\n\u001B[1;32m    846\u001B[0m \u001B[38;5;66;03m# see #1639\u001B[39;00m\n\u001B[0;32m--> 847\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m adapter_layer_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madapter_layer_names \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mother_param_names:\n\u001B[1;32m    848\u001B[0m     adapter_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, adapter_layer_name, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    849\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(adapter_layer, (nn\u001B[38;5;241m.\u001B[39mModuleDict, nn\u001B[38;5;241m.\u001B[39mParameterDict, BufferDict)):\n",
      "\u001B[0;31mTypeError\u001B[0m: can only concatenate str (not \"tuple\") to str"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d2d0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if \"vera_lambda_\" in n], \"lr\": vera_lr},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if \"classifier\" in n], \"lr\": head_lr},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Instantiate scheduler\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.06 * (len(train_dataloader) * num_epochs),\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dd5aa8-977b-4ac0-8b96-884b17bcdd00",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa0e73be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:18<00:00,  1.58it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'accuracy': 0.7475490196078431, 'f1': 0.8367670364500792}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.68it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'accuracy': 0.7671568627450981, 'f1': 0.8536209553158706}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.66it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: {'accuracy': 0.8553921568627451, 'f1': 0.8959435626102292}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.64it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: {'accuracy': 0.8823529411764706, 'f1': 0.9133574007220215}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:17<00:00,  1.63it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: {'accuracy': 0.8897058823529411, 'f1': 0.9183303085299456}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch.to(device)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        predictions, references = predictions, batch[\"labels\"]\n",
    "        metric.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "        )\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    print(f\"epoch {epoch}:\", eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b2caca",
   "metadata": {},
   "source": [
    "## Share adapters on the ðŸ¤— Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b23af6f-cf6e-486f-9d10-0eada95b631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = ...  # your Hugging Face Hub account ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "990b3c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/BenjaminB/roberta-large-peft-vera/commit/d720cdc67b97df9cd1453b14d3e0fb17efc8779b', commit_message='Upload model', commit_description='', oid='d720cdc67b97df9cd1453b14d3e0fb17efc8779b', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(f\"{account_id}/roberta-large-peft-vera\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d140b26",
   "metadata": {},
   "source": [
    "## Load adapters from the Hub\n",
    "\n",
    "You can also directly load adapters from the Hub using the commands below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c283e028-b349-46b0-a20e-cde0ee5fbd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "320b10a0-4ea8-4786-9f3c-4670019c6b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = f\"{account_id}/roberta-large-peft-vera\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "inference_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3a94049-bc01-4f2e-8cf9-66daf24a4402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Vera model\n",
    "inference_model = PeftModel.from_pretrained(inference_model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd919fef-4e9a-4dc5-a957-7b879cfc5d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                     | 0/4 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8480392156862745, 'f1': 0.8938356164383561}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference_model.to(device)\n",
    "inference_model.eval()\n",
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = inference_model(**batch)\n",
    "    predictions = outputs.logits.argmax(dim=-1)\n",
    "    predictions, references = predictions, batch[\"labels\"]\n",
    "    metric.add_batch(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "    )\n",
    "\n",
    "eval_metric = metric.compute()\n",
    "print(eval_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc0125-a052-4e26-b7ff-af0f763be34e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
